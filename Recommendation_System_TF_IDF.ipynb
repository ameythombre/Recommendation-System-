{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk        \n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# import string\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmaAdj = []\n",
    "lemmaVerb = []\n",
    "tagged_tokens = []\n",
    "token = []\n",
    "\n",
    "# wordnet and treebank have different tagging systems\n",
    "# define a mapping between wordnet tags and POS tags as a function\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    # if pos tag starts with 'J'\n",
    "    if pos_tag.startswith('J'):\n",
    "        # return wordnet tag \"ADJ\"\n",
    "        return wordnet.ADJ\n",
    "\n",
    "    # if pos tag starts with 'V'\n",
    "    elif pos_tag.startswith('V'):\n",
    "        # return wordnet tag \"VERB\"\n",
    "        return wordnet.VERB\n",
    "\n",
    "    # if pos tag starts with 'N'\n",
    "    elif pos_tag.startswith('N'):\n",
    "        # return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # be default, return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def tokenize(text):\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    token_count = None\n",
    "    for i in re.findall(\"([A-Z]+)\", str(text)):\n",
    "        text = text.replace(i, i.lower())\n",
    "    \n",
    "    \n",
    "    text = re.findall(r'[a-z0-9][a-z0-9._@-]*[a-z0-9]', str(text))\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    tagged_tokens= nltk.pos_tag(text)\n",
    "    \n",
    "\n",
    "    lemmatized_words=[wordnet_lemmatizer.lemmatize\\\n",
    "          (word, get_wordnet_pos(tag)) \\\n",
    "          for (word, tag) in tagged_tokens \\\n",
    "          # remove stop words\n",
    "              if word not in stop_words and \\\n",
    "          # remove punctuations\n",
    "              word not in string.punctuation]\n",
    "    #print(lemmatized_words)\n",
    "    word_dist=nltk.FreqDist(lemmatized_words)\n",
    "    return word_dist\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def get_tf_idf(text):\n",
    "    docs_tokens={idx:tokenize(doc) \\\n",
    "             for idx,doc in enumerate(text)}\n",
    "    \n",
    "    \n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\" )\n",
    "    dtm=dtm.fillna(0)\n",
    "    #print(dtm)\n",
    "    \n",
    "    tf=dtm.values\n",
    "    doc_len=tf.sum(axis=1)\n",
    "    tf=np.divide(tf.T, doc_len).T\n",
    "    \n",
    "    df=np.where(tf>0,1,0)\n",
    "    \n",
    "    smoothed_idf=np.log(np.divide(len(text)+1, np.sum(df, axis=0)+1))+1    \n",
    "    smoothed_tf_idf=tf*smoothed_idf\n",
    "    \n",
    "    return smoothed_tf_idf, docs_tokens\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "\n",
    "    data = pd.read_csv(\"entireBookList8700.csv\", header=0)\n",
    "    tf_idf,freq_dist= get_tf_idf(data[\"Description\"].values.tolist())\n",
    "    print(\"TF_IDF Matrix for description of all 8700 books:\\n\",tf_idf)\n",
    "    print(\"\\nFrequency distribution\\n\", freq_dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
