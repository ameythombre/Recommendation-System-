{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIA-660 Web Mining Project\n",
    "## Web Scraping  Goodreads Website\n",
    "## Project Title : Recommendation System\n",
    "\n",
    "![alt text](http://alexabloom.com/wp-content/uploads/2017/04/Goodreads-icon.png \"Logo Title Text 1\")\n",
    "\n",
    "### The link for the website: [www.goodreads.com](https://www.goodreads.com/list/tag/)\n",
    "### Objectives: \n",
    "Our project for BIA-660 Web Mining is going to focus on \"Goodreads Books\". We started this project to solve both a Business problem as well as to satisfy the user needs. This project focuses on the following:\n",
    "*\tCategorizing the books using the classification methods (Naive Bayes, Support vector machine (SVM) & KNN), when a new book is added to the database, our model predicts which category the book belongs to.\n",
    "*\tRecommending books to the user based on the user reviews (Category Based - Review Based).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the tags (Categories) & Links\n",
    "Initally, we are scraping the 29 tags (categories of the books) and their links to go inside each category and browse for the books which belong to that category.\n",
    "* Using BeautifulSoup to scrape the data from the Goodreads website. \n",
    "* Using \"Request\" library to retrive the source code.\n",
    "* Initializing a function called \"getCategoryHref\" to get the Cetegory links.\n",
    "* Writing the contents to the \"categoryListGR.csv\" File.\n",
    "\n",
    "[categoryListGR.csv](https://github.com/MukunthR/recomendation-system/blob/master/categoryListGR.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# import BeautifulSoup from package bs4 (i.e. beautifulsoup4)\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = { 'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.'\n",
    "                          '86 Safari/537.36'}\n",
    "\n",
    "page = requests.get('https://www.goodreads.com/list/tag/', headers = headers)\n",
    "\n",
    "# send a get request to the web page\n",
    "siteUrl = \"https://www.goodreads.com\"\n",
    "category = {}\n",
    "\n",
    "def getCategoryHref(url):\n",
    "    page = requests.get(url, headers = headers)\n",
    "    if page.status_code == 200:\n",
    "        # initiate a beautifulsoup object using the html source and Pythonâ€™s html.parser\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        hrefTag = soup.select('div#topRow > div.cell a.listTitle')\n",
    "        for href in hrefTag:\n",
    "            finalHref = siteUrl + href['href']\n",
    "            break\n",
    "        return finalHref    \n",
    "\n",
    "if page.status_code == 200:\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#     print(soup.prettify())\n",
    "    categoryNameTags = soup.select('div ul.listTagsTwoColumn li a.actionLinkLite')\n",
    "    for name in categoryNameTags:\n",
    "        print(name)\n",
    "        categoryHref = getCategoryHref(siteUrl + name['href'])\n",
    "        category[name.text.strip()] = categoryHref\n",
    "    \n",
    "    with open('categoryListGR.csv', 'w') as file:\n",
    "        wrt = csv.writer(file, delimiter=',', lineterminator='\\n')\n",
    "        wrt.writerow(['Category', 'Link'])\n",
    "        for key,value in category.items():\n",
    "            wrt.writerow([key,value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching books from each category with their Scores and Links\n",
    "Each category from the previous step is called for and the data for the first 300 books are scrapped. This dataset includes Category, Names, Links and Scores attributes.\n",
    "* Creating a List each attribute and combining them to form Pandas Dataframe.\n",
    "* Using categoryListGR.csv file to fetch the data of the inidividual book. \n",
    "* Writing the contents to the \"categoryBookFile.csv\" File.\n",
    "\n",
    " [categoryBookFile.csv](https://github.com/MukunthR/recomendation-system/blob/master/categoryBookFile.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# import BeautifulSoup from package bs4 (i.e. beautifulsoup4)\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = { 'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.'\n",
    "                          '86 Safari/537.36'}\n",
    "BookName = []\n",
    "bookLink = []\n",
    "Page_No = []\n",
    "bookTupList = []\n",
    "Score = []\n",
    "pairWiseScore = []\n",
    "flagList = []\n",
    "flagCount = 0\n",
    "catCustom = []\n",
    "\n",
    "siteUrl = \"https://www.goodreads.com\"\n",
    "def pairwise(it):\n",
    "    it = iter(it)\n",
    "    while True:\n",
    "        yield next(it), next(it)\n",
    "        \n",
    "def scrapeData(category, url):\n",
    "    print(category)\n",
    "    page = requests.get(url, headers = headers)\n",
    "    if page.status_code == 200:\n",
    "        \n",
    "        print(\"page success\")\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        bookName = soup.select('a.bookTitle span')\n",
    "        for i in bookName:\n",
    "            BookName.append(i.text)\n",
    "            catCustom.append(category)\n",
    "        link = soup.select('a.bookTitle')\n",
    "        for i in link:\n",
    "            bookLink.append(siteUrl + i['href'])\n",
    "        scr = soup.select('div span.smallText > a')\n",
    "        for i in scr:\n",
    "            Score.append(i.text.strip())\n",
    "        for a,b in pairwise(Score):\n",
    "            pairWiseScore.append(a + b)\n",
    "        flagList = list(zip(catCustom,BookName,bookLink,pairWiseScore))\n",
    "        if (len(flagList) % 300) != 0:\n",
    "            print(len(flagList))\n",
    "            Page = soup.select('div.pagination a.next_page')\n",
    "            if Page:\n",
    "                for i in Page:\n",
    "                    scrapeData(category, siteUrl + i[\"href\"])\n",
    "        return list(zip(catCustom,BookName,bookLink,pairWiseScore))\n",
    "          \n",
    "\n",
    "def scrapteCategoryData(categoryList):\n",
    "    for category,categoryHref in categoryList.items():\n",
    "        print(category,categoryHref)\n",
    "        scrapeData(category, categoryHref)\n",
    "        \n",
    "    entireList = list(zip(catCustom,BookName,bookLink,pairWiseScore))\n",
    "    for element in entireList:\n",
    "        print(element)\n",
    "    final_data = pd.DataFrame(entireList, columns = ['Category','Name','Link','Scores'])\n",
    "    print(final_data)\n",
    "    final_data.to_csv('categoryBookFile.csv', index = False)\n",
    "    \n",
    "    print(\"Total length is :: \" + str(len(entireList)))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    data = pd.read_csv(\"categoryListGR.csv\", header=0)\n",
    "    categoryList = dict(zip(data[\"Category\"].values.tolist(), data[\"Link\"].values.tolist()))\n",
    "    scrapteCategoryData(categoryList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the contents of the books\n",
    "The content of all the 300 books of each category is taken and stored in a csv file.\n",
    "*  Reading the entire csv file, fetch the movie links columns and convert it into list.\n",
    "* Iterate each movie links from the list and fetch book contents such as Name, Author Name, Author profile link, Ratings, Awards, Descriptions and which other category it belongs to.\n",
    "\n",
    "[entireBookList1000.csv](https://github.com/MukunthR/recomendation-system/blob/master/entireBookList1000.csv)\n",
    "\n",
    "[entireBookList8700.csv](https://github.com/MukunthR/recomendation-system/blob/master/entireBookList8700.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = { 'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.'\n",
    "                          '86 Safari/537.36'}\n",
    "\n",
    "entireBookList = []\n",
    "\n",
    "def scrapeData(link, category, score):\n",
    "    OtherCategories = []\n",
    "    page = requests.get(link, headers = headers)\n",
    "    if page.status_code == 200:\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        \n",
    "        bookTag = soup.select('div h1#bookTitle')\n",
    "        if not bookTag:\n",
    "            bookName = \"N/A\"\n",
    "        else:\n",
    "            for tag in bookTag:\n",
    "                bookName = tag.text.strip()\n",
    "        \n",
    "        authorTag = soup.select('div a.authorName')\n",
    "        if not authorTag:\n",
    "            authorName = \"N/A\"\n",
    "        else:\n",
    "            for author in authorTag:\n",
    "                authorName = author.text.strip()\n",
    "        \n",
    "        authorLinkTag = soup.select('div a.authorName')\n",
    "        if not authorLinkTag:\n",
    "            authorLink = \"N/A\"\n",
    "        else:\n",
    "            for i in authorLinkTag:\n",
    "                authorLink = i[\"href\"]\n",
    "        \n",
    "        ratingTag = soup.select('span.average')\n",
    "        if not ratingTag:\n",
    "            rating = \"N/A\"\n",
    "        else:\n",
    "            for i in ratingTag:\n",
    "                rating = i.text\n",
    "        \n",
    "        awardsTag = soup.select('div.infoBoxRowItem a.award')\n",
    "        if not awardsTag:\n",
    "            awards = 'N/A'\n",
    "        else:\n",
    "            for i in awardsTag:\n",
    "                awards = i.text.strip()\n",
    "        \n",
    "        othrcat = soup.select('div.elementList a.actionLinkLite')\n",
    "        if not othrcat:\n",
    "            othrCat = \"N/A\"\n",
    "        else:\n",
    "            for i in othrcat:\n",
    "                OtherCategories.append(i.text.strip())\n",
    "            othrCat = \" \".join(OtherCategories)\n",
    "        \n",
    "        \n",
    "        descriptionTag = soup.select('div#description span')\n",
    "        if not descriptionTag:\n",
    "            description = \"N/A\"\n",
    "        else:\n",
    "            for i in descriptionTag:\n",
    "                description = i.text\n",
    "        \n",
    "        \n",
    "        bookTuple = (category, bookName, authorName, authorLink, rating, awards, othrCat, description,score)\n",
    "        entireBookList.append(bookTuple)\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_csv(\"categoryBookFile.csv\", header=0)\n",
    "    movieLinkList = data[\"Link\"].tolist()\n",
    "    movieCategories = data[\"Category\"].tolist()\n",
    "    movieScores = data[\"Scores\"].tolist()\n",
    "    j = 0\n",
    "    for link, category, score in zip(movieLinkList, movieCategories, movieScores):\n",
    "        j += 1\n",
    "        print(j)\n",
    "        print(link, category, score)\n",
    "        scrapeData(link, category, score)\n",
    "        \n",
    "    for element in entireBookList:\n",
    "        print(element)\n",
    "        \n",
    "    final_data = pd.DataFrame(entireBookList, columns = ['Category','Book Name','Author Name','Author Link','Ratings', 'Awards', 'OtherLinks', 'Description', 'Score'])\n",
    "    final_data.to_csv('entireBookList8700.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization & Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk        \n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# nltk.download('genesis')\n",
    "# nltk.download('punkt')\n",
    "# # # nltk.download('all')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial import distance\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# import string\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmaAdj = []\n",
    "lemmaVerb = []\n",
    "tagged_tokens = []\n",
    "token = []\n",
    "\n",
    "# wordnet and treebank have different tagging systems\n",
    "# define a mapping between wordnet tags and POS tags as a function\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    # if pos tag starts with 'J'\n",
    "    if pos_tag.startswith('J'):\n",
    "        # return wordnet tag \"ADJ\"\n",
    "        return wordnet.ADJ\n",
    "\n",
    "    # if pos tag starts with 'V'\n",
    "    elif pos_tag.startswith('V'):\n",
    "        # return wordnet tag \"VERB\"\n",
    "        return wordnet.VERB\n",
    "\n",
    "    # if pos tag starts with 'N'\n",
    "    elif pos_tag.startswith('N'):\n",
    "        # return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # be default, return wordnet tag \"NOUN\"\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def tokenize(text):\n",
    "#     print(text)\n",
    "    token_count = None\n",
    "    \n",
    "    # converts the string into lowercase\n",
    "#     for i in text:\n",
    "#         low_text = i.lower()\n",
    "\n",
    "    # segments the lowercased string into tokens - Based on the requirements\n",
    "    pattern = r'[a-z0-9][a-z0-9-_.@]*[a-z0-9]'\n",
    "    tokens = nltk.regexp_tokenize(text, pattern)\n",
    "\n",
    "    # POS\n",
    "    tagged_tokens= nltk.pos_tag(tokens)\n",
    "\n",
    "    lemmatized_words=[wordnet_lemmatizer.lemmatize\\\n",
    "          (word, get_wordnet_pos(tag)) \\\n",
    "          # tagged_tokens is a list of tuples (word, tag)\n",
    "          for (word, tag) in tagged_tokens \\\n",
    "          # remove stop words\n",
    "          if word not in stop_words and \\\n",
    "          # remove punctuations\n",
    "          word not in string.punctuation]\n",
    "\n",
    "    token_count = nltk.FreqDist(lemmatized_words)\n",
    "    \n",
    "    return token_count \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "#     text=\n",
    "    data = pd.read_csv(\"entireBookList1000.csv\", header=0)\n",
    "    text = data[\"Description\"].tolist()\n",
    "    \n",
    "    tokenize(text)\n",
    "    for key, value in tokenize(text).items():\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF_IDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
